{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of a2c_cont.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyck33/reinforcement-learning/blob/master/Copy_of_a2c_cont.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCSvpjD3EAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This one works\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from keras.layers import Dense, Input, Lambda\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "EPISODES = 3000\n",
        "\n",
        "\n",
        "\n",
        "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
        "class A2CAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # if you want to see Cartpole learning, then change to True\n",
        "        self.render = False\n",
        "        self.load_model = False\n",
        "        self.state_size = state_size #3\n",
        "        self.action_size = action_size #1\n",
        "        self.value_size = 1\n",
        "\n",
        "        # get gym environment name\n",
        "        # these are hyper parameters for the A3C\n",
        "        self.actor_lr = 0.0001\n",
        "        self.critic_lr = 0.001\n",
        "        self.discount_factor = .9\n",
        "        self.hidden1, self.hidden2 = 24, 24\n",
        "\n",
        "        # create model for actor and critic network\n",
        "        self.actor, self.critic = self.build_model()\n",
        "\n",
        "        # method for training actor and critic network\n",
        "        self.optimizer = [self.actor_optimizer(), self.critic_optimizer()]\n",
        "\n",
        "        if self.load_model:\n",
        "            self.actor.load_weights(\"./saved_models/PendulumK.h5\")\n",
        "            self.critic.load_weights(\"./saved_models/PendulumK.h5\")\n",
        "\n",
        "    def build_model(self):\n",
        "        state = Input(batch_shape=(None, self.state_size))\n",
        "        actor_input = Dense(30, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform')(state)\n",
        "        # actor_hidden = Dense(self.hidden2, activation='relu')(actor_input)\n",
        "        #tanh output of [-1,1]\n",
        "        mu_0 = Dense(self.action_size, activation='tanh', kernel_initializer='he_uniform')(actor_input)\n",
        "        #softplus gives output [0, inf] and deriv is sigmoid\n",
        "        sigma_0 = Dense(self.action_size, activation='softplus', kernel_initializer='he_uniform')(actor_input)\n",
        "        #mu is doubled to fit the action space of [-2, 2]?\n",
        "        mu = Lambda(lambda x: x * 2)(mu_0)\n",
        "        #custom layer ensures that sigma is not 0\n",
        "        sigma = Lambda(lambda x: x + 0.0001)(sigma_0)\n",
        "        #critic also takes in state and outputs a value\n",
        "        critic_input = Dense(30, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform')(state)\n",
        "        # value_hidden = Dense(self.hidden2, activation='relu')(critic_input)\n",
        "        state_value = Dense(1, activation='linear', kernel_initializer='he_uniform')(critic_input)\n",
        "\n",
        "        actor = Model(inputs=state, outputs=(mu, sigma))\n",
        "        critic = Model(inputs=state, outputs=state_value)\n",
        "        #must declare before use\n",
        "        actor._make_predict_function()\n",
        "        critic._make_predict_function()\n",
        "\n",
        "        actor.summary()\n",
        "        critic.summary()\n",
        "\n",
        "        return actor, critic\n",
        "\n",
        "    def actor_optimizer(self):\n",
        "        #placeholders for actions and advantages parameters coming in\n",
        "        action = K.placeholder(shape=(None, 1))\n",
        "        advantages = K.placeholder(shape=(None, 1))\n",
        "\n",
        "        # mu = K.placeholder(shape=(None, self.action_size))\n",
        "        # sigma_sq = K.placeholder(shape=(None, self.action_size))\n",
        "\n",
        "        mu, sigma_sq = self.actor.output\n",
        "\n",
        "        #defined a custom loss using PDF formula, K.exp is element-wise exponential\n",
        "        pdf = 1. / K.sqrt(2. * np.pi * sigma_sq) * K.exp(-K.square(action - mu) / (2. * sigma_sq))\n",
        "        #log pdf why?\n",
        "        log_pdf = K.log(pdf + K.epsilon())\n",
        "        #entropy looks different from log(sqrt(2 * pi * e * sigma_sq))\n",
        "        #Sum of the values in a tensor, alongside the specified axis.\n",
        "        entropy = K.sum(0.5 * (K.log(2. * np.pi * sigma_sq) + 1.))\n",
        "\n",
        "        exp_v = log_pdf * advantages\n",
        "        #entropy is made small before added to exp_v\n",
        "        exp_v = K.sum(exp_v + 0.01 * entropy)\n",
        "        #loss is a negation\n",
        "        actor_loss = -exp_v\n",
        "\n",
        "        #use custom loss to perform updates with Adam, ie. get gradients\n",
        "        optimizer = Adam(lr=self.actor_lr)\n",
        "        updates = optimizer.get_updates(self.actor.trainable_weights, [], actor_loss)\n",
        "        #adjust params with custom train function\n",
        "        train = K.function([self.actor.input, action, advantages], [], updates=updates)\n",
        "        #return custom train function\n",
        "        return train\n",
        "\n",
        "    # make loss function for Value approximation\n",
        "    def critic_optimizer(self):\n",
        "        #placeholder for parameter target that comes in\n",
        "        discounted_reward = K.placeholder(shape=(None, 1))\n",
        "        #get output\n",
        "        value = self.critic.output\n",
        "        #MSE loss\n",
        "        loss = K.mean(K.square(discounted_reward - value))\n",
        "\n",
        "        optimizer = Adam(lr=self.critic_lr)\n",
        "        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)\n",
        "        train = K.function([self.critic.input, discounted_reward], [], updates=updates)\n",
        "        return train\n",
        "\n",
        "    # using the output of policy network, pick action stochastically\n",
        "    def get_action(self, state):\n",
        "        mu, sigma_sq = self.actor.predict(np.reshape(state, [1, self.state_size]))\n",
        "        # sigma_sq = np.log(np.exp(sigma_sq + 1))\n",
        "        #return sample from std normal distribution \n",
        "        #epsilon is the random factor with mu and sigma_sq\n",
        "        epsilon = np.random.randn(self.action_size)\n",
        "        # action = norm.rvs(loc=mu, scale=sigma_sq,size=1)\n",
        "        #mean + (std * epsilon)\n",
        "        action = mu + np.sqrt(sigma_sq) * epsilon\n",
        "        action = np.clip(action, -2, 2)\n",
        "        return action\n",
        "\n",
        "    # update policy network every episode\n",
        "    #inputs of S,A,R,S'\n",
        "    #potential output of advantages and target\n",
        "    def train_model(self, state, action, reward, next_state, done):\n",
        "        target = np.zeros((1, self.value_size))\n",
        "        advantages = np.zeros((1, self.action_size))\n",
        "\n",
        "        value = self.critic.predict(state)[0]\n",
        "        next_value = self.critic.predict(next_state)[0]\n",
        "\n",
        "        if done:\n",
        "            advantages[0] = reward - value\n",
        "            target[0][0] = reward\n",
        "        else:\n",
        "            advantages[0] = reward + self.discount_factor * (next_value) - value\n",
        "            target[0][0] = reward + self.discount_factor * next_value\n",
        "\n",
        "        #self.optimizer = [self.actor_optimizer(), self.critic_optimizer()]\n",
        "        #so [0] is tot train actor \n",
        "        self.optimizer[0]([state, action, advantages])\n",
        "        #[1] trains critic\n",
        "        self.optimizer[1]([state, target])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # In case of CartPole-v1, maximum length of episode is 500\n",
        "    env = gym.make('Pendulum-v0')\n",
        "    # get size of state and action from environment\n",
        "    state_size = env.observation_space.shape[0] #3\n",
        "    action_size = env.action_space.shape[0] #1\n",
        "\n",
        "    # make A2C agent\n",
        "    agent = A2CAgent(state_size, action_size)\n",
        "\n",
        "    scores, episodes = [], []\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        done = False\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "\n",
        "        while not done:\n",
        "            if agent.render:\n",
        "                env.render()\n",
        "\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            reward /= 10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            # if an action make the episode end, then gives penalty of -100\n",
        "            agent.train_model(state, action, reward, next_state, done)\n",
        "\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                # every episode, plot the play time\n",
        "                scores.append(score)\n",
        "                episodes.append(e)\n",
        "                print(\"episode:\", e, \"  score:\", score)\n",
        "\n",
        "                # if the mean of scores of last 10 episode is bigger than 490\n",
        "                # stop training\n",
        "                if np.mean(scores[-min(10, len(scores)):]) > -20:\n",
        "                    sys.exit()\n",
        "\n",
        "        # save the model\n",
        "        if e % 50 == 0:\n",
        "            agent.actor.save_weights(\"PendulumK_actor.h5\")\n",
        "            agent.critic.save_weights(\"PendulumK_critic.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqiNW5r3LhLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A2C continuous action space \n",
        "import sys\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "from math import e #2.718281...\n",
        "from scipy.stats import norm\n",
        "import keras\n",
        "from keras.layers import Dense, Input, Lambda\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "GAMMA = 0.9\n",
        "REWARD_STEPS = 2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 5e-5\n",
        "ENTROPY_BETA = 1e-4\n",
        "\n",
        "class Actor:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.actor_lr = 0.0001\n",
        "        self.hidden_size = 32\n",
        "\n",
        "        self.actor = self.build_actor()\n",
        "\n",
        "        self.optimizer = self.actor_optimizer()\n",
        "        \n",
        "    #input: state\n",
        "    #output mu and sigma_sq\n",
        "    def build_actor(self):\n",
        "\n",
        "        #base for two heads of mean and variance\n",
        "        base = Input(batch_shape=(None, self.state_size), name='states')\n",
        "        net = Dense(units=self.hidden_size, use_bias=False, activation='relu')(base)\n",
        "\n",
        "        #mu head\n",
        "        mu = Dense(units=1, activation='tanh')(net)\n",
        "        #custom layer for Pendulum\n",
        "        mu = Lambda(lambda x: x * 2)(mu)\n",
        "\n",
        "        #sigma head\n",
        "        sigma_sq = Dense(units=1, activation='softplus')(net)\n",
        "        #custom layer to ensure non-zero variance\n",
        "        sigma_sq = Lambda(lambda x:x +0.0001)(sigma_sq)\n",
        "\n",
        "        actor = Model(inputs=base, outputs=(mu, sigma_sq))\n",
        "\n",
        "        #prep the function\n",
        "        actor._make_predict_function()\n",
        "\n",
        "        actor.summary()\n",
        "        \n",
        "        return actor\n",
        "    #params: action, advantage\n",
        "    #advantage calculated in train fn before optimizer called\n",
        "    #use PDF to calculate loss\n",
        "    #textbook uses sum of three losses\n",
        "    def actor_optimizer(self):\n",
        "        action = K.placeholder(shape=(None, 1))\n",
        "        advantages = K.placeholder(shape=(None, 1))\n",
        "        #self.model.outputs\n",
        "        mu, sigma_sq = self.actor.output\n",
        "        #mu, sigma_sq = self.actor.predict(state)\n",
        "        #entropy of Gaussian\n",
        "        entropy_loss = ENTROPY_BETA * (-K.mean(0.5 * (K.log(2. * np.pi * sigma_sq) + 1.)))\n",
        "        \n",
        "        #Prob Density Fn (PDF)\n",
        "        #if sigma_sq is not None:\n",
        "        #problem with clip, don't use TF tensor as bool error\n",
        "            #sigma_sq = np.clip(sigma_sq,1e-3, None)\n",
        "        p1 = - ((action - mu) ** 2) / (2 * K.clip(sigma_sq, 1e-3, None)) #clip min only\n",
        "        p2 = - K.log(K.sqrt(2 * np.pi * sigma_sq))\n",
        "        #log prob(a|s) given theta\n",
        "        log_prob = p1 + p2\n",
        "        #log_prob * score fn = advantage\n",
        "        log_prob_v = advantages * log_prob\n",
        "        loss_policy_v = -K.mean(log_prob_v) \n",
        "        #sum losses\n",
        "        loss_v = loss_policy_v + entropy_loss \n",
        "        optimizer = Adam(lr=self.actor_lr)\n",
        "        updates = optimizer.get_updates(self.actor.trainable_weights, [], loss_v )\n",
        "        train = K.function([self.actor.input, action, advantages], [], updates=updates)\n",
        "\n",
        "        return train\n",
        "\n",
        "class Critic:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.value_size = 1\n",
        "        self.hidden_size = 32\n",
        "        self.critic_lr = 0.001\n",
        "        self.critic = self.build_critic()\n",
        "        self.optimizer = self.critic_optimizer()\n",
        "\n",
        "    def build_critic(self):\n",
        "        states = Input(batch_shape=(None, self.state_size,))\n",
        "\n",
        "        net = Dense(units=self.hidden_size, activation='relu')(states)\n",
        "\n",
        "        value = Dense(units=self.value_size, activation='linear')(net)\n",
        "        #models.Model?\n",
        "        critic = Model(inputs=states, outputs=value)\n",
        "\n",
        "        #optimizer = optimizer.Adam(lr=self.critic_lr)\n",
        "        critic.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=self.critic_lr))\n",
        "        critic.summary()\n",
        "\n",
        "        return critic\n",
        "\n",
        "    def get_targets_adv(self, state, action, reward, next_state, done):\n",
        "        #value_size = 1, action_size = 1\n",
        "        target = np.zeros((1, self.value_size))\n",
        "        advantages = np.zeros((1, self.action_size))\n",
        "        #why element [0]\n",
        "        value = self.critic.predict(state)[0]\n",
        "        next_value = self.critic.predict(next_state)[0]\n",
        "\n",
        "        if done:\n",
        "            advantages[0] = reward - value\n",
        "            target[0][0] = reward\n",
        "        else:\n",
        "            advantages[0] = reward + GAMMA * (next_value) - value\n",
        "            target[0][0] = reward + GAMMA * next_value\n",
        "\n",
        "        return target, advantages\n",
        "\n",
        "    #need a critic optimizer and custom train fn to use target\n",
        "    #minus value output of critic as base of MSE loss\n",
        "    #from get_targets_adv\n",
        "    \n",
        "    def critic_optimizer(self):\n",
        "\n",
        "        disc_reward = K.placeholder(shape=(None,1))\n",
        "        #output of critic\n",
        "        value = self.critic.output\n",
        "        #MSE error\n",
        "        loss = K.mean(K.square(disc_reward - value))\n",
        "\n",
        "        optimizer = Adam(lr=self.critic_lr)\n",
        "        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)\n",
        "        train = K.function([self.critic.input, disc_reward], [], updates=updates)\n",
        "        return train\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RKdt8tQLhlE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fdb07470-6a62-4b05-b5af-eef9b64b1171"
      },
      "source": [
        "#Agent file for A2C continuous\n",
        "import sys\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "from math import e #2.718281...\n",
        "from scipy.stats import norm\n",
        "from keras.layers import Dense, Input, Lambda\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "\n",
        "#from a2c_contin import Actor, Critic\n",
        "\n",
        "ENV_ID = \"Pendulum-v0\"\n",
        "GAMMA = 0.9\n",
        "REWARD_STEPS = 2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 5e-5\n",
        "ENTROPY_BETA = 1e-4\n",
        "\n",
        "EPISODES = 10000\n",
        "\n",
        "class A2CAgent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.render = False\n",
        "        self.load_model = False\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.actor = Actor(self.state_size, self.action_size)\n",
        "\n",
        "        self.critic = Critic(self.state_size, self.action_size)\n",
        "\n",
        "        self.test = 1\n",
        "\n",
        "        #self.actor_optimizer = actor_optimizer()\n",
        "        #self.critic_optimizer = critic_optimizer()\n",
        "\n",
        "        if self.load_model:\n",
        "            self.actor.load_weights('pendulum_actor.h5')\n",
        "            self.critic.load_weights('pendulum_critic.h5')\n",
        "\n",
        "    #vs get_action for Pendulum using epsilon\n",
        "    def act(self, state):\n",
        "        state = np.reshape(state, [-1, self.state_size])\n",
        "        mu, sigma_sq = self.actor.actor.predict(state)\n",
        "        action = np.random.normal(mu, np.sqrt(sigma_sq))\n",
        "        #just in case action is an extreme outlier beyond 68-95-99.7\n",
        "        #clip it at the action range\n",
        "        action = np.clip(action, -2, 2) #clip at [-2,2] for Pendulum\n",
        "        return action\n",
        "\n",
        "    def train_model(self, state, action, target, advantages):\n",
        "        \n",
        "        #TODO: must separate mu and sigma actors to train fit them\n",
        "        self.actor.optimizer([state, action, advantages])\n",
        "        \n",
        "        self.critic.optimizer([state, target])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    env = gym.make(ENV_ID)\n",
        "\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.shape[0]\n",
        "\n",
        "    agent = A2CAgent(state_size, action_size)\n",
        "\n",
        "    print(agent.test)\n",
        "\n",
        "    scores, episodes = [], []\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        done = False\n",
        "        score = 0\n",
        "        mean = 0\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "\n",
        "        while not done:\n",
        "            if agent.render:\n",
        "                env.render()\n",
        "\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            #reward /= 10\n",
        "\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "            target, advantages = agent.critic.get_targets_adv(state, action, reward, next_state, done)\n",
        "\n",
        "            agent.train_model(state, action, target, advantages)\n",
        "\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "\n",
        "                scores.append(score)\n",
        "                episodes.append(e)\n",
        "                mean = np.mean(scores[-min(10, len(scores)):])\n",
        "                print(\"episode:\", e, \"  score:\", score, \"  mean:\", mean)\n",
        "\n",
        "                if np.mean(scores[-min(10, len(scores)):]) > -200:\n",
        "                    sys.exit()\n",
        "        if e % 50 == 0:\n",
        "            agent.actor.actor.save_weights('pendulum_agent.h5')\n",
        "            agent.critic.critic.save_weights('pendulum_critic.h5')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "states (InputLayer)             (None, 3)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 32)           96          states[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1)            33          dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 1)            33          dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 1)            0           dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 1)            0           dense_18[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 162\n",
            "Trainable params: 162\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 161\n",
            "Trainable params: 161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1\n",
            "episode: 0   score: [-1339.94379242]   mean: -1339.9437924160986\n",
            "episode: 1   score: [-1193.54423636]   mean: -1266.7440143903086\n",
            "episode: 2   score: [-1675.40961268]   mean: -1402.9658804864641\n",
            "episode: 3   score: [-1743.91104273]   mean: -1488.202171047184\n",
            "episode: 4   score: [-1203.17053269]   mean: -1431.1958433754305\n",
            "episode: 5   score: [-1366.62063939]   mean: -1420.4333093782789\n",
            "episode: 6   score: [-1144.62927016]   mean: -1381.0327323473937\n",
            "episode: 7   score: [-1303.14418695]   mean: -1371.2966641728872\n",
            "episode: 8   score: [-1386.45713573]   mean: -1372.9811610128359\n",
            "episode: 9   score: [-1073.39190443]   mean: -1343.0222353541283\n",
            "episode: 10   score: [-1241.64709093]   mean: -1333.1925652051027\n",
            "episode: 11   score: [-894.85017199]   mean: -1303.3231587674786\n",
            "episode: 12   score: [-1569.36379083]   mean: -1292.7185765824365\n",
            "episode: 13   score: [-1359.96129322]   mean: -1254.3236016317226\n",
            "episode: 14   score: [-1482.42564021]   mean: -1282.2491123843636\n",
            "episode: 15   score: [-1779.4885616]   mean: -1323.5359046046547\n",
            "episode: 16   score: [-1376.05795423]   mean: -1346.6787730116437\n",
            "episode: 17   score: [-1326.78862251]   mean: -1349.043216567458\n",
            "episode: 18   score: [-1497.82199504]   mean: -1360.1797024980099\n",
            "episode: 19   score: [-1647.66369474]   mean: -1417.6068815294961\n",
            "episode: 20   score: [-1337.8333855]   mean: -1427.225510987026\n",
            "episode: 21   score: [-1388.62040767]   mean: -1476.6025345553971\n",
            "episode: 22   score: [-1514.86354692]   mean: -1471.1525101650145\n",
            "episode: 23   score: [-1489.85222905]   mean: -1484.14160374806\n",
            "episode: 24   score: [-1603.93900107]   mean: -1496.2929398331294\n",
            "episode: 25   score: [-1454.11848937]   mean: -1463.7559326107498\n",
            "episode: 26   score: [-1534.11564032]   mean: -1479.5617012192135\n",
            "episode: 27   score: [-1795.02768645]   mean: -1526.3856076137085\n",
            "episode: 28   score: [-1815.1046388]   mean: -1558.1138719894755\n",
            "episode: 29   score: [-1727.63000743]   mean: -1566.1105032585342\n",
            "episode: 30   score: [-1783.51292979]   mean: -1610.6784576878974\n",
            "episode: 31   score: [-1791.14018797]   mean: -1650.9304357176727\n",
            "episode: 32   score: [-1661.54031583]   mean: -1665.5981126080085\n",
            "episode: 33   score: [-1475.70555959]   mean: -1664.1834456612753\n",
            "episode: 34   score: [-1773.3548117]   mean: -1681.1250267249293\n",
            "episode: 35   score: [-1793.41182816]   mean: -1715.0543606041501\n",
            "episode: 36   score: [-1785.39482697]   mean: -1740.182279269507\n",
            "episode: 37   score: [-1847.76912481]   mean: -1745.4564231049894\n",
            "episode: 38   score: [-1593.89918718]   mean: -1723.3358779430455\n",
            "episode: 39   score: [-1515.23133306]   mean: -1702.0960105061822\n",
            "episode: 40   score: [-1871.0633255]   mean: -1710.8510500763653\n",
            "episode: 41   score: [-1847.51165861]   mean: -1716.4881971404152\n",
            "episode: 42   score: [-1748.50154703]   mean: -1725.1843202608004\n",
            "episode: 43   score: [-1838.40413537]   mean: -1761.4541778387797\n",
            "episode: 44   score: [-1612.03267631]   mean: -1745.3219643000652\n",
            "episode: 45   score: [-1683.0993796]   mean: -1734.2907194435168\n",
            "episode: 46   score: [-1475.83398819]   mean: -1703.3346355655933\n",
            "episode: 47   score: [-1469.45354127]   mean: -1665.5030772113223\n",
            "episode: 48   score: [-1506.71859684]   mean: -1656.7850181774659\n",
            "episode: 49   score: [-1778.05402655]   mean: -1683.0672875262517\n",
            "episode: 50   score: [-1751.99210776]   mean: -1671.1601657527267\n",
            "episode: 51   score: [-1779.97944946]   mean: -1664.4069448373516\n",
            "episode: 52   score: [-1827.86564853]   mean: -1672.3433549875506\n",
            "episode: 53   score: [-1830.66931168]   mean: -1671.569872618587\n",
            "episode: 54   score: [-1753.59561773]   mean: -1685.72616676009\n",
            "episode: 55   score: [-1207.80474368]   mean: -1638.196703168223\n",
            "episode: 56   score: [-1578.78944466]   mean: -1648.4922488152563\n",
            "episode: 57   score: [-1706.99018708]   mean: -1672.2459133967616\n",
            "episode: 58   score: [-1680.09186858]   mean: -1689.5832405713675\n",
            "episode: 59   score: [-1503.1405281]   mean: -1662.091890725874\n",
            "episode: 60   score: [-1480.85118848]   mean: -1634.9777987972434\n",
            "episode: 61   score: [-1431.90595869]   mean: -1600.1704497207206\n",
            "episode: 62   score: [-1581.65886455]   mean: -1575.549771322235\n",
            "episode: 63   score: [-1808.75440495]   mean: -1573.3582806499194\n",
            "episode: 64   score: [-1782.52982098]   mean: -1576.2517009749815\n",
            "episode: 65   score: [-1462.11770191]   mean: -1601.6829967975962\n",
            "episode: 66   score: [-1652.92431217]   mean: -1609.096483548075\n",
            "episode: 67   score: [-1373.60667159]   mean: -1575.7581319988728\n",
            "episode: 68   score: [-1423.86205005]   mean: -1550.1351501451832\n",
            "episode: 69   score: [-1445.31436864]   mean: -1544.3525341991933\n",
            "episode: 70   score: [-1449.08024984]   mean: -1541.1754403360594\n",
            "episode: 71   score: [-1405.59080366]   mean: -1538.5439248331447\n",
            "episode: 72   score: [-1410.50623717]   mean: -1521.4286620954822\n",
            "episode: 73   score: [-1517.34161118]   mean: -1492.2873827181695\n",
            "episode: 74   score: [-1423.17375413]   mean: -1456.3517760335124\n",
            "episode: 75   score: [-1425.78944714]   mean: -1452.7189505573565\n",
            "episode: 76   score: [-1451.69308324]   mean: -1432.595827664522\n",
            "episode: 77   score: [-1725.43200708]   mean: -1467.7783612132896\n",
            "episode: 78   score: [-1309.56186588]   mean: -1456.348342796891\n",
            "episode: 79   score: [-1438.87173946]   mean: -1455.7040798790144\n",
            "episode: 80   score: [-1841.93424323]   mean: -1494.9894792173513\n",
            "episode: 81   score: [-1130.38539551]   mean: -1467.4689384019987\n",
            "episode: 82   score: [-1324.52312438]   mean: -1458.870627123042\n",
            "episode: 83   score: [-1122.08413545]   mean: -1419.344879550193\n",
            "episode: 84   score: [-1194.06356167]   mean: -1396.4338603034223\n",
            "episode: 85   score: [-1321.01862684]   mean: -1385.9567782728002\n",
            "episode: 86   score: [-1203.59605642]   mean: -1361.1470755910782\n",
            "episode: 87   score: [-1295.77861792]   mean: -1318.1817366758203\n",
            "episode: 88   score: [-1134.45449964]   mean: -1300.671000051277\n",
            "episode: 89   score: [-1013.31036047]   mean: -1258.114862152632\n",
            "episode: 90   score: [-1078.02648982]   mean: -1181.7240868122817\n",
            "episode: 91   score: [-987.25993344]   mean: -1167.4115406055596\n",
            "episode: 92   score: [-900.73797016]   mean: -1125.0330251830098\n",
            "episode: 93   score: [-992.74483855]   mean: -1112.0990954925633\n",
            "episode: 94   score: [-1031.92747444]   mean: -1095.8854867700375\n",
            "episode: 95   score: [-1171.59779543]   mean: -1080.9434036296504\n",
            "episode: 96   score: [-765.99603608]   mean: -1037.1834015960853\n",
            "episode: 97   score: [-748.17325971]   mean: -982.4228657747606\n",
            "episode: 98   score: [-1132.82999039]   mean: -982.2604148498904\n",
            "episode: 99   score: [-1199.01492572]   mean: -1000.8308713750208\n",
            "episode: 100   score: [-1055.09270794]   mean: -998.5374931863228\n",
            "episode: 101   score: [-1223.43885771]   mean: -1022.1553856135133\n",
            "episode: 102   score: [-889.66260833]   mean: -1021.0478494306408\n",
            "episode: 103   score: [-891.58833684]   mean: -1010.9321992596679\n",
            "episode: 104   score: [-1258.44738934]   mean: -1033.5841907494855\n",
            "episode: 105   score: [-802.50064793]   mean: -996.6744759989928\n",
            "episode: 106   score: [-1026.44190355]   mean: -1022.7190627452825\n",
            "episode: 107   score: [-1051.56977591]   mean: -1053.058714365144\n",
            "episode: 108   score: [-1153.9793393]   mean: -1055.1736492564291\n",
            "episode: 109   score: [-927.37916825]   mean: -1028.0100735090348\n",
            "episode: 110   score: [-1119.84712545]   mean: -1034.4855152600812\n",
            "episode: 111   score: [-1173.1696472]   mean: -1029.4585942092044\n",
            "episode: 112   score: [-867.70499308]   mean: -1027.2628326840777\n",
            "episode: 113   score: [-1091.63002908]   mean: -1047.2670019086968\n",
            "episode: 114   score: [-770.61056055]   mean: -998.4833190294181\n",
            "episode: 115   score: [-881.84406123]   mean: -1006.4176603592207\n",
            "episode: 116   score: [-1570.85016618]   mean: -1060.85848662286\n",
            "episode: 117   score: [-1135.77259162]   mean: -1069.2787681936238\n",
            "episode: 118   score: [-1289.25109149]   mean: -1082.8059434124054\n",
            "episode: 119   score: [-752.60145825]   mean: -1065.328172412425\n",
            "episode: 120   score: [-1145.5101879]   mean: -1067.8944786572747\n",
            "episode: 121   score: [-882.26768404]   mean: -1038.8042823404267\n",
            "episode: 122   score: [-1103.85186734]   mean: -1062.4189697667023\n",
            "episode: 123   score: [-1245.04310151]   mean: -1077.7602770097287\n",
            "episode: 124   score: [-1070.92230689]   mean: -1107.7914516442365\n",
            "episode: 125   score: [-1022.28364195]   mean: -1121.835409716349\n",
            "episode: 126   score: [-1323.10429179]   mean: -1097.0608222768394\n",
            "episode: 127   score: [-1362.70754384]   mean: -1119.7543174987863\n",
            "episode: 128   score: [-638.55429244]   mean: -1054.6846375935525\n",
            "episode: 129   score: [-1372.09579555]   mean: -1116.6340713237382\n",
            "episode: 130   score: [-1203.6109135]   mean: -1122.4441438844162\n",
            "episode: 131   score: [-635.65566492]   mean: -1097.7829419726086\n",
            "episode: 132   score: [-634.91733376]   mean: -1050.889488614685\n",
            "episode: 133   score: [-767.71823184]   mean: -1003.1570016472436\n",
            "episode: 134   score: [-881.96525225]   mean: -984.2612961826844\n",
            "episode: 135   score: [-906.62561835]   mean: -972.6954938225997\n",
            "episode: 136   score: [-647.4192824]   mean: -905.1269928838708\n",
            "episode: 137   score: [-1011.27644704]   mean: -869.9838832046519\n",
            "episode: 138   score: [-766.22847812]   mean: -882.7513017734166\n",
            "episode: 139   score: [-656.39212099]   mean: -811.1809343169787\n",
            "episode: 140   score: [-763.59039062]   mean: -767.1788820282469\n",
            "episode: 141   score: [-1344.77038756]   mean: -838.0903542921545\n",
            "episode: 142   score: [-857.95254512]   mean: -860.3938754285033\n",
            "episode: 143   score: [-906.66702386]   mean: -874.2887546308206\n",
            "episode: 144   score: [-668.70873548]   mean: -852.9631029541249\n",
            "episode: 145   score: [-829.93863743]   mean: -845.2944048626832\n",
            "episode: 146   score: [-891.28710715]   mean: -869.6811873378867\n",
            "episode: 147   score: [-1091.25507076]   mean: -877.6790497095699\n",
            "episode: 148   score: [-907.53211658]   mean: -891.8094135554945\n",
            "episode: 149   score: [-1350.22251612]   mean: -961.1924530686945\n",
            "episode: 150   score: [-1075.10962044]   mean: -992.344376051083\n",
            "episode: 151   score: [-911.9615972]   mean: -949.063497014926\n",
            "episode: 152   score: [-602.48010158]   mean: -923.516252660902\n",
            "episode: 153   score: [-1198.79728974]   mean: -952.7292792490756\n",
            "episode: 154   score: [-1088.46654453]   mean: -994.7050601544761\n",
            "episode: 155   score: [-1051.40219126]   mean: -1016.8514155370997\n",
            "episode: 156   score: [-1013.45247737]   mean: -1029.0679525588089\n",
            "episode: 157   score: [-1105.73400678]   mean: -1030.5158461610745\n",
            "episode: 158   score: [-1045.64555038]   mean: -1044.3271895403109\n",
            "episode: 159   score: [-1111.58899673]   mean: -1020.4638376018653\n",
            "episode: 160   score: [-1015.03580466]   mean: -1014.4564560235806\n",
            "episode: 161   score: [-1006.86257803]   mean: -1023.9465541074617\n",
            "episode: 162   score: [-1177.31549049]   mean: -1081.4300929981637\n",
            "episode: 163   score: [-1213.48051047]   mean: -1082.8984150710091\n",
            "episode: 164   score: [-1192.58750321]   mean: -1093.3105109387404\n",
            "episode: 165   score: [-1231.49272487]   mean: -1111.319564299597\n",
            "episode: 166   score: [-1357.93826359]   mean: -1145.7681429211602\n",
            "episode: 167   score: [-1339.67800918]   mean: -1169.1625431602565\n",
            "episode: 168   score: [-1328.83697895]   mean: -1197.4816860181047\n",
            "episode: 169   score: [-1073.59995458]   mean: -1193.6827818030438\n",
            "episode: 170   score: [-924.60128325]   mean: -1184.63932966215\n",
            "episode: 171   score: [-844.34106645]   mean: -1168.3871785042015\n",
            "episode: 172   score: [-1054.27600495]   mean: -1156.083229950058\n",
            "episode: 173   score: [-944.02579815]   mean: -1129.13775871818\n",
            "episode: 174   score: [-653.22159164]   mean: -1075.2011675614863\n",
            "episode: 175   score: [-644.56172791]   mean: -1016.5080678658176\n",
            "episode: 176   score: [-1214.17851775]   mean: -1002.1320932819393\n",
            "episode: 177   score: [-1214.45061306]   mean: -989.6093536699849\n",
            "episode: 178   score: [-966.88922114]   mean: -953.4145778882091\n",
            "episode: 179   score: [-1115.90067294]   mean: -957.6446497242111\n",
            "episode: 180   score: [-1327.14856567]   mean: -997.8993779664404\n",
            "episode: 181   score: [-1039.07044298]   mean: -1017.3723156191409\n",
            "episode: 182   score: [-1019.15079522]   mean: -1013.8597946459134\n",
            "episode: 183   score: [-1061.75156165]   mean: -1025.632370995139\n",
            "episode: 184   score: [-937.71800311]   mean: -1054.0820121415156\n",
            "episode: 185   score: [-1047.61963884]   mean: -1094.387803234525\n",
            "episode: 186   score: [-932.51417824]   mean: -1066.2213692838932\n",
            "episode: 187   score: [-1062.45963148]   mean: -1051.0222711264796\n",
            "episode: 188   score: [-932.66512568]   mean: -1047.5998615810245\n",
            "episode: 189   score: [-1078.15187535]   mean: -1043.8249818220133\n",
            "episode: 190   score: [-844.14541554]   mean: -995.5246668088075\n",
            "episode: 191   score: [-775.76105183]   mean: -969.1937276940218\n",
            "episode: 192   score: [-1217.28440324]   mean: -989.0070884966268\n",
            "episode: 193   score: [-906.313343]   mean: -973.4632666320513\n",
            "episode: 194   score: [-647.17890793]   mean: -944.4093571146446\n",
            "episode: 195   score: [-914.39682635]   mean: -931.087075865277\n",
            "episode: 196   score: [-648.23548814]   mean: -902.6592068549414\n",
            "episode: 197   score: [-1271.0041059]   mean: -923.5136542967059\n",
            "episode: 198   score: [-639.33777206]   mean: -894.1809189339926\n",
            "episode: 199   score: [-1141.08063101]   mean: -900.4737945000379\n",
            "episode: 200   score: [-778.9797375]   mean: -893.9572266964451\n",
            "episode: 201   score: [-548.28856211]   mean: -871.2099777236392\n",
            "episode: 202   score: [-649.11955329]   mean: -814.3934927285914\n",
            "episode: 203   score: [-940.93047228]   mean: -817.8552056563467\n",
            "episode: 204   score: [-663.60856938]   mean: -819.4981718010762\n",
            "episode: 205   score: [-1191.60987785]   mean: -847.2194769509281\n",
            "episode: 206   score: [-681.8117495]   mean: -850.5771030868173\n",
            "episode: 207   score: [-1094.58112992]   mean: -832.9348054886432\n",
            "episode: 208   score: [-1231.1013639]   mean: -892.1111646729854\n",
            "episode: 209   score: [-544.50486053]   mean: -832.4535876251224\n",
            "episode: 210   score: [-1081.4089108]   mean: -862.6965049550088\n",
            "episode: 211   score: [-1053.07689398]   mean: -913.1753381425336\n",
            "episode: 212   score: [-992.09506649]   mean: -947.4728894619209\n",
            "episode: 213   score: [-934.39983801]   mean: -946.8198260356154\n",
            "episode: 214   score: [-978.50455105]   mean: -978.3094242026733\n",
            "episode: 215   score: [-775.91876783]   mean: -936.7403132011725\n",
            "episode: 216   score: [-655.72587001]   mean: -934.1317252525569\n",
            "episode: 217   score: [-717.27607124]   mean: -896.4012193851174\n",
            "episode: 218   score: [-1024.9208717]   mean: -875.783170165462\n",
            "episode: 219   score: [-646.2066786]   mean: -885.953351972179\n",
            "episode: 220   score: [-788.73884733]   mean: -856.6863456253847\n",
            "episode: 221   score: [-646.22797696]   mean: -816.0014539235347\n",
            "episode: 222   score: [-1141.77776034]   mean: -830.9697233085832\n",
            "episode: 223   score: [-907.24083836]   mean: -828.2538233429634\n",
            "episode: 224   score: [-784.45314714]   mean: -808.8486829518181\n",
            "episode: 225   score: [-666.92386398]   mean: -797.9491925668569\n",
            "episode: 226   score: [-1090.83358269]   mean: -841.4599638344419\n",
            "episode: 227   score: [-1004.81764748]   mean: -870.2141214580708\n",
            "episode: 228   score: [-961.15328087]   mean: -863.8373623745085\n",
            "episode: 229   score: [-594.17121868]   mean: -858.6338163820095\n",
            "episode: 230   score: [-720.71204619]   mean: -851.8311362676843\n",
            "episode: 231   score: [-993.79634069]   mean: -886.587972640659\n",
            "episode: 232   score: [-1085.82389188]   mean: -880.9925857948549\n",
            "episode: 233   score: [-1202.87223309]   mean: -910.555725267782\n",
            "episode: 234   score: [-1061.61497148]   mean: -938.2719077021857\n",
            "episode: 235   score: [-1091.15932898]   mean: -980.6954542019696\n",
            "episode: 236   score: [-816.34461061]   mean: -953.2465569938362\n",
            "episode: 237   score: [-1084.16551668]   mean: -961.1813439138189\n",
            "episode: 238   score: [-1100.23173196]   mean: -975.0891890227806\n",
            "episode: 239   score: [-652.04253299]   mean: -980.876320453873\n",
            "episode: 240   score: [-1227.0974892]   mean: -1031.5148647546043\n",
            "episode: 241   score: [-701.5319793]   mean: -1002.2884286157508\n",
            "episode: 242   score: [-1186.75180782]   mean: -1012.3812202095669\n",
            "episode: 243   score: [-967.58488194]   mean: -988.8524850947788\n",
            "episode: 244   score: [-1147.5103015]   mean: -997.4420180969015\n",
            "episode: 245   score: [-948.99033169]   mean: -983.2251183674787\n",
            "episode: 246   score: [-952.10730623]   mean: -996.8013879298105\n",
            "episode: 247   score: [-990.68248862]   mean: -987.4530851238858\n",
            "episode: 248   score: [-1222.10534061]   mean: -999.6404459894557\n",
            "episode: 249   score: [-1200.98260967]   mean: -1054.5344536578154\n",
            "episode: 250   score: [-920.56380904]   mean: -1023.8810856421663\n",
            "episode: 251   score: [-1073.28007526]   mean: -1061.055895237805\n",
            "episode: 252   score: [-879.23717826]   mean: -1030.3044322820385\n",
            "episode: 253   score: [-817.90794425]   mean: -1015.3367385128533\n",
            "episode: 254   score: [-837.05683263]   mean: -984.2913916250336\n",
            "episode: 255   score: [-1073.62317386]   mean: -996.7546758425217\n",
            "episode: 256   score: [-956.16054224]   mean: -997.1599994433552\n",
            "episode: 257   score: [-929.34141397]   mean: -991.0258919786047\n",
            "episode: 258   score: [-1066.95803796]   mean: -975.5111617129303\n",
            "episode: 259   score: [-934.23601065]   mean: -948.8365018107731\n",
            "episode: 260   score: [-781.71403814]   mean: -934.9515247207394\n",
            "episode: 261   score: [-1129.82265408]   mean: -940.6057826028476\n",
            "episode: 262   score: [-1191.86645064]   mean: -971.8687098411438\n",
            "episode: 263   score: [-1374.99509891]   mean: -1027.5774253071884\n",
            "episode: 264   score: [-1116.79285007]   mean: -1055.5510270517902\n",
            "episode: 265   score: [-1382.55340171]   mean: -1086.4440498369372\n",
            "episode: 266   score: [-649.89867222]   mean: -1055.8178628354867\n",
            "episode: 267   score: [-1086.40715471]   mean: -1071.5244369088487\n",
            "episode: 268   score: [-733.88931634]   mean: -1038.2175647475694\n",
            "episode: 269   score: [-625.5388432]   mean: -1007.3478480022384\n",
            "episode: 270   score: [-999.24426391]   mean: -1029.1008705789352\n",
            "episode: 271   score: [-655.49464595]   mean: -981.6680697653986\n",
            "episode: 272   score: [-1217.5723029]   mean: -984.2386549913193\n",
            "episode: 273   score: [-1103.27897882]   mean: -957.0670429829812\n",
            "episode: 274   score: [-991.36258512]   mean: -944.5240164876902\n",
            "episode: 275   score: [-1212.17731234]   mean: -927.48640755064\n",
            "episode: 276   score: [-1089.85435963]   mean: -971.4819762918063\n",
            "episode: 277   score: [-853.74102914]   mean: -948.2153637353304\n",
            "episode: 278   score: [-682.48960555]   mean: -943.075392656601\n",
            "episode: 279   score: [-576.33236651]   mean: -938.1547449879101\n",
            "episode: 280   score: [-568.29200105]   mean: -895.0595187026763\n",
            "episode: 281   score: [-754.31723012]   mean: -904.9417771200409\n",
            "episode: 282   score: [-680.34168748]   mean: -851.2187155773943\n",
            "episode: 283   score: [-499.64290054]   mean: -790.8551077487091\n",
            "episode: 284   score: [-1109.23104291]   mean: -802.6419535273819\n",
            "episode: 285   score: [-642.82328477]   mean: -745.7065507705238\n",
            "episode: 286   score: [-517.3146399]   mean: -688.4525787967352\n",
            "episode: 287   score: [-1231.25943668]   mean: -726.2044195507896\n",
            "episode: 288   score: [-908.49655124]   mean: -748.8051141197134\n",
            "episode: 289   score: [-1061.2439052]   mean: -797.2962679883977\n",
            "episode: 290   score: [-388.84568291]   mean: -779.3516361740228\n",
            "episode: 291   score: [-829.71278476]   mean: -786.8911916384047\n",
            "episode: 292   score: [-386.58319706]   mean: -757.5153425967776\n",
            "episode: 293   score: [-517.10634259]   mean: -759.2616868016063\n",
            "episode: 294   score: [-803.6574154]   mean: -728.7043240514328\n",
            "episode: 295   score: [-395.32611683]   mean: -703.9546072572923\n",
            "episode: 296   score: [-653.58763525]   mean: -717.5819067925746\n",
            "episode: 297   score: [-553.5901703]   mean: -649.8149801539457\n",
            "episode: 298   score: [-896.72371823]   mean: -648.6376968528264\n",
            "episode: 299   score: [-793.33553076]   mean: -621.8468594089051\n",
            "episode: 300   score: [-679.38385208]   mean: -650.9006763257879\n",
            "episode: 301   score: [-829.97535817]   mean: -650.9269336668568\n",
            "episode: 302   score: [-767.28532654]   mean: -688.997146614555\n",
            "episode: 303   score: [-522.81451034]   mean: -689.5679633896254\n",
            "episode: 304   score: [-793.74624113]   mean: -688.5768459624795\n",
            "episode: 305   score: [-505.77601387]   mean: -699.6218356668694\n",
            "episode: 306   score: [-391.01987334]   mean: -673.365059476413\n",
            "episode: 307   score: [-509.1792381]   mean: -668.9239662569795\n",
            "episode: 308   score: [-429.39178378]   mean: -622.1907728113124\n",
            "episode: 309   score: [-396.16207363]   mean: -582.473427098281\n",
            "episode: 310   score: [-535.85775687]   mean: -568.120817576924\n",
            "episode: 311   score: [-527.64698615]   mean: -537.8879803744763\n",
            "episode: 312   score: [-798.87159312]   mean: -541.0466070332792\n",
            "episode: 313   score: [-642.99950378]   mean: -553.0651063774092\n",
            "episode: 314   score: [-391.27820044]   mean: -512.818302307676\n",
            "episode: 315   score: [-394.42680799]   mean: -501.68338171872284\n",
            "episode: 316   score: [-823.28408415]   mean: -544.9098027997162\n",
            "episode: 317   score: [-943.04814671]   mean: -588.2966936601723\n",
            "episode: 318   score: [-657.71698803]   mean: -611.1292140857904\n",
            "episode: 319   score: [-387.15519813]   mean: -610.2285265357852\n",
            "episode: 320   score: [-458.42596832]   mean: -602.4853476813746\n",
            "episode: 321   score: [-789.36669661]   mean: -628.6573187270615\n",
            "episode: 322   score: [-529.11285462]   mean: -601.6814448762609\n",
            "episode: 323   score: [-873.48557239]   mean: -624.7300517376018\n",
            "episode: 324   score: [-1003.37673801]   mean: -685.9399054950488\n",
            "episode: 325   score: [-791.16603333]   mean: -725.6138280295894\n",
            "episode: 326   score: [-1083.16372948]   mean: -751.6017925618056\n",
            "episode: 327   score: [-1234.65000547]   mean: -780.7619784384954\n",
            "episode: 328   score: [-1212.73204462]   mean: -836.2634840975649\n",
            "episode: 329   score: [-1095.86083602]   mean: -907.1340478874006\n",
            "episode: 330   score: [-392.47352307]   mean: -900.5388033625195\n",
            "episode: 331   score: [-394.77862598]   mean: -861.0799962995083\n",
            "episode: 332   score: [-953.14015813]   mean: -903.4827266508937\n",
            "episode: 333   score: [-1068.83603169]   mean: -923.0177725803351\n",
            "episode: 334   score: [-906.08468146]   mean: -913.2885669250232\n",
            "episode: 335   score: [-1056.29446236]   mean: -939.8014098278449\n",
            "episode: 336   score: [-645.09994993]   mean: -895.9950318734873\n",
            "episode: 337   score: [-940.5355022]   mean: -866.583581545911\n",
            "episode: 338   score: [-526.0415795]   mean: -797.9145350333703\n",
            "episode: 339   score: [-799.42818942]   mean: -768.271270373352\n",
            "episode: 340   score: [-799.04933247]   mean: -808.9288513128104\n",
            "episode: 341   score: [-972.72598111]   mean: -866.7235868264588\n",
            "episode: 342   score: [-950.38078838]   mean: -866.4476498519216\n",
            "episode: 343   score: [-399.17397241]   mean: -799.4814439242289\n",
            "episode: 344   score: [-725.48866368]   mean: -781.4218421468024\n",
            "episode: 345   score: [-526.15344337]   mean: -728.4077402478813\n",
            "episode: 346   score: [-824.95805681]   mean: -746.3935509351235\n",
            "episode: 347   score: [-799.27128354]   mean: -732.2671290695178\n",
            "episode: 348   score: [-395.02148774]   mean: -719.1651198939709\n",
            "episode: 349   score: [-662.40876811]   mean: -705.4631777626892\n",
            "episode: 350   score: [-827.57752476]   mean: -708.3159969922445\n",
            "episode: 351   score: [-945.64661811]   mean: -705.6080606915955\n",
            "episode: 352   score: [-754.16581391]   mean: -685.9865632446254\n",
            "episode: 353   score: [-657.16557885]   mean: -711.7857238886346\n",
            "episode: 354   score: [-423.25246753]   mean: -681.5621042738223\n",
            "episode: 355   score: [-660.88830801]   mean: -695.0355907373925\n",
            "episode: 356   score: [-515.94418553]   mean: -664.134203609862\n",
            "episode: 357   score: [-538.67457037]   mean: -638.0745322924673\n",
            "episode: 358   score: [-935.91131205]   mean: -692.1635147232639\n",
            "episode: 359   score: [-792.50860784]   mean: -705.1734986962676\n",
            "episode: 360   score: [-791.49117241]   mean: -701.5648634611625\n",
            "episode: 361   score: [-530.48134578]   mean: -660.0483362283137\n",
            "episode: 362   score: [-788.29391011]   mean: -663.46114584803\n",
            "episode: 363   score: [-644.42922703]   mean: -662.1875106659863\n",
            "episode: 364   score: [-277.77544602]   mean: -647.639808514775\n",
            "episode: 365   score: [-534.29369149]   mean: -634.9803468628581\n",
            "episode: 366   score: [-259.70987071]   mean: -609.3569153804751\n",
            "episode: 367   score: [-521.48594686]   mean: -607.6380530296602\n",
            "episode: 368   score: [-627.82519443]   mean: -576.8294412672941\n",
            "episode: 369   score: [-902.24753067]   mean: -587.8033335505854\n",
            "episode: 370   score: [-745.04150034]   mean: -583.158366343672\n",
            "episode: 371   score: [-264.39606281]   mean: -556.549838046697\n",
            "episode: 372   score: [-628.58224789]   mean: -540.5786718244865\n",
            "episode: 373   score: [-387.16030954]   mean: -514.8517800756526\n",
            "episode: 374   score: [-740.53561858]   mean: -561.1277973313593\n",
            "episode: 375   score: [-674.25349848]   mean: -575.1237780303366\n",
            "episode: 376   score: [-408.43387454]   mean: -589.9961784136385\n",
            "episode: 377   score: [-592.63889614]   mean: -597.1114733415586\n",
            "episode: 378   score: [-1040.4756883]   mean: -638.3765227284376\n",
            "episode: 379   score: [-1060.76428631]   mean: -654.2281982916168\n",
            "episode: 380   score: [-977.66681535]   mean: -677.4907297918771\n",
            "episode: 381   score: [-608.81573914]   mean: -711.9326974249972\n",
            "episode: 382   score: [-536.38081721]   mean: -702.7125543565093\n",
            "episode: 383   score: [-540.45639205]   mean: -718.04216260715\n",
            "episode: 384   score: [-896.19517143]   mean: -733.60811789192\n",
            "episode: 385   score: [-662.11976855]   mean: -732.3947448996621\n",
            "episode: 386   score: [-812.45734377]   mean: -772.7970918231746\n",
            "episode: 387   score: [-531.97663989]   mean: -766.7308661979699\n",
            "episode: 388   score: [-703.56849239]   mean: -733.0401466071717\n",
            "episode: 389   score: [-796.92336283]   mean: -706.6560542595396\n",
            "episode: 390   score: [-876.47047732]   mean: -696.5364204567252\n",
            "episode: 391   score: [-263.56005089]   mean: -662.0108516322701\n",
            "episode: 392   score: [-777.85878204]   mean: -686.1586481159143\n",
            "episode: 393   score: [-893.76261466]   mean: -721.4892703768317\n",
            "episode: 394   score: [-260.18585658]   mean: -657.8883388919797\n",
            "episode: 395   score: [-792.70812465]   mean: -670.9471745016706\n",
            "episode: 396   score: [-949.32679646]   mean: -684.6341197701344\n",
            "episode: 397   score: [-743.27707122]   mean: -705.7641629033699\n",
            "episode: 398   score: [-395.73030572]   mean: -674.9803442371118\n",
            "episode: 399   score: [-720.75453289]   mean: -667.3634612432663\n",
            "episode: 400   score: [-651.01306527]   mean: -644.8177200385726\n",
            "episode: 401   score: [-388.777377]   mean: -657.3394526491385\n",
            "episode: 402   score: [-561.13519]   mean: -635.6670934452394\n",
            "episode: 403   score: [-1.82661037]   mean: -546.4734930162806\n",
            "episode: 404   score: [-788.78015958]   mean: -599.3329233158862\n",
            "episode: 405   score: [-262.55970671]   mean: -546.3180815215563\n",
            "episode: 406   score: [-948.82648539]   mean: -546.2680504148564\n",
            "episode: 407   score: [-269.83727884]   mean: -498.9240711772468\n",
            "episode: 408   score: [-395.31668675]   mean: -498.88270927952925\n",
            "episode: 409   score: [-660.84887382]   mean: -492.8921433727781\n",
            "episode: 410   score: [-535.53189614]   mean: -481.34402645968976\n",
            "episode: 411   score: [-393.48159542]   mean: -481.8144483021395\n",
            "episode: 412   score: [-132.70296586]   mean: -438.97122588797885\n",
            "episode: 413   score: [-540.0885744]   mean: -492.7974222908814\n",
            "episode: 414   score: [-132.96105385]   mean: -427.21551171861876\n",
            "episode: 415   score: [-394.06237005]   mean: -440.36577805290824\n",
            "episode: 416   score: [-264.77125056]   mean: -371.96025456989616\n",
            "episode: 417   score: [-645.36385726]   mean: -409.5129124114043\n",
            "episode: 418   score: [-526.21819438]   mean: -422.60306317480155\n",
            "episode: 419   score: [-527.76806976]   mean: -409.29498276814195\n",
            "episode: 420   score: [-622.28477527]   mean: -417.9702706807293\n",
            "episode: 421   score: [-528.94736111]   mean: -431.5168472488989\n",
            "episode: 422   score: [-262.3988437]   mean: -444.48643503255954\n",
            "episode: 423   score: [-562.5661206]   mean: -446.73418965328585\n",
            "episode: 424   score: [-573.81211501]   mean: -490.819295769342\n",
            "episode: 425   score: [-686.36505708]   mean: -520.0495644724854\n",
            "episode: 426   score: [-801.37565472]   mean: -573.7100048885911\n",
            "episode: 427   score: [-392.48665201]   mean: -548.4222843641201\n",
            "episode: 428   score: [-259.40503219]   mean: -521.7409681453368\n",
            "episode: 429   score: [-502.06296078]   mean: -519.1704572479051\n",
            "episode: 430   score: [-1028.89519213]   mean: -559.831498934727\n",
            "episode: 431   score: [-132.87382713]   mean: -520.2241455375349\n",
            "episode: 432   score: [-1021.31863924]   mean: -596.1161250913683\n",
            "episode: 433   score: [-1.06629362]   mean: -539.9661423927001\n",
            "episode: 434   score: [-1081.11456904]   mean: -590.6963877952527\n",
            "episode: 435   score: [-399.58094842]   mean: -562.0179769293711\n",
            "episode: 436   score: [-802.03965153]   mean: -562.0843766103583\n",
            "episode: 437   score: [-1104.88232036]   mean: -633.3239434452211\n",
            "episode: 438   score: [-1170.10338078]   mean: -724.3937783041247\n",
            "episode: 439   score: [-1200.89092465]   mean: -794.2765746910125\n",
            "episode: 440   score: [-135.44378647]   mean: -704.931434124168\n",
            "episode: 441   score: [-417.80238079]   mean: -733.4242894898375\n",
            "episode: 442   score: [-265.82098225]   mean: -657.8745237907764\n",
            "episode: 443   score: [-971.50217425]   mean: -754.9181118540764\n",
            "episode: 444   score: [-1073.14488546]   mean: -754.1211434963145\n",
            "episode: 445   score: [-1268.82558173]   mean: -841.0456068274391\n",
            "episode: 446   score: [-1225.66720708]   mean: -883.4083623821037\n",
            "episode: 447   score: [-252.853102]   mean: -798.2054405464496\n",
            "episode: 448   score: [-400.69818818]   mean: -721.2649212866525\n",
            "episode: 449   score: [-933.14980806]   mean: -694.4908096269647\n",
            "episode: 450   score: [-267.92132098]   mean: -707.7385630783534\n",
            "episode: 451   score: [-1098.31109009]   mean: -775.7894340077241\n",
            "episode: 452   score: [-522.97987245]   mean: -801.505323027727\n",
            "episode: 453   score: [-390.66568722]   mean: -743.4216743248105\n",
            "episode: 454   score: [-403.36079041]   mean: -676.4432648193144\n",
            "episode: 455   score: [-266.94425982]   mean: -576.255132628055\n",
            "episode: 456   score: [-135.2584163]   mean: -467.21425355042754\n",
            "episode: 457   score: [-265.58753327]   mean: -468.4876966764576\n",
            "episode: 458   score: [-263.94593273]   mean: -454.812471131337\n",
            "episode: 459   score: [-261.36433998]   mean: -387.6339243242172\n",
            "episode: 460   score: [-265.74317448]   mean: -387.41610967394007\n",
            "episode: 461   score: [-406.49159741]   mean: -318.2341604061092\n",
            "episode: 462   score: [-1037.0646898]   mean: -369.6426421418957\n",
            "episode: 463   score: [-1056.97421191]   mean: -436.27349461052245\n",
            "episode: 464   score: [-267.66309803]   mean: -422.70372537328296\n",
            "episode: 465   score: [-3.36303752]   mean: -396.3456031430161\n",
            "episode: 466   score: [-522.9468049]   mean: -435.11444200229397\n",
            "episode: 467   score: [-130.41207467]   mean: -421.59689614238584\n",
            "episode: 468   score: [-1065.62432764]   mean: -501.76473563274504\n",
            "episode: 469   score: [-131.56759242]   mean: -488.7850608764762\n",
            "episode: 470   score: [-821.99055101]   mean: -544.4097985296919\n",
            "episode: 471   score: [-1117.75932144]   mean: -615.5365709333728\n",
            "episode: 472   score: [-407.66404895]   mean: -552.5965068485051\n",
            "episode: 473   score: [-1152.46829875]   mean: -562.1459155330767\n",
            "episode: 474   score: [-134.32720431]   mean: -548.8123261611001\n",
            "episode: 475   score: [-410.24137412]   mean: -589.5001598216302\n",
            "episode: 476   score: [-1275.03735987]   mean: -664.7092153187552\n",
            "episode: 477   score: [-259.9966673]   mean: -677.6676745819997\n",
            "episode: 478   score: [-703.04587168]   mean: -641.4098289864789\n",
            "episode: 479   score: [-1108.41828952]   mean: -739.0948986965749\n",
            "episode: 480   score: [-1252.94480187]   mean: -782.1903237831613\n",
            "episode: 481   score: [-1101.6713372]   mean: -780.5815253588794\n",
            "episode: 482   score: [-1188.26510687]   mean: -858.6416311502717\n",
            "episode: 483   score: [-267.3249892]   mean: -770.1273001951704\n",
            "episode: 484   score: [-918.95034374]   mean: -848.5896141377913\n",
            "episode: 485   score: [-677.32279785]   mean: -875.2977565102877\n",
            "episode: 486   score: [-979.62766432]   mean: -845.7567869550912\n",
            "episode: 487   score: [-856.74241718]   mean: -905.431361943069\n",
            "episode: 488   score: [-687.56013264]   mean: -903.8827880387868\n",
            "episode: 489   score: [-139.18727317]   mean: -806.9596864031109\n",
            "episode: 490   score: [-402.87700265]   mean: -721.9529064810928\n",
            "episode: 491   score: [-528.69786707]   mean: -664.6555594684372\n",
            "episode: 492   score: [-128.39443232]   mean: -558.668492013316\n",
            "episode: 493   score: [-407.16437187]   mean: -572.6524302799313\n",
            "episode: 494   score: [-533.01431465]   mean: -534.0588273710047\n",
            "episode: 495   score: [-657.01420531]   mean: -532.0279681168989\n",
            "episode: 496   score: [-135.25295011]   mean: -447.5904966960526\n",
            "episode: 497   score: [-533.27897526]   mean: -415.2441525038841\n",
            "episode: 498   score: [-932.63071951]   mean: -439.7512111904589\n",
            "episode: 499   score: [-406.72496051]   mean: -466.50497992522094\n",
            "episode: 500   score: [-3.36355923]   mean: -426.55363558249735\n",
            "episode: 501   score: [-401.36501301]   mean: -413.8203501756758\n",
            "episode: 502   score: [-562.81623574]   mean: -457.2625305180339\n",
            "episode: 503   score: [-267.17091358]   mean: -443.26318468970004\n",
            "episode: 504   score: [-126.54954822]   mean: -402.61670804691306\n",
            "episode: 505   score: [-896.09019492]   mean: -426.5243070083358\n",
            "episode: 506   score: [-411.7103613]   mean: -454.17004812727566\n",
            "episode: 507   score: [-533.68104503]   mean: -454.2102551048809\n",
            "episode: 508   score: [-936.65680162]   mean: -454.6128633163348\n",
            "episode: 509   score: [-394.42310114]   mean: -453.38267737916766\n",
            "episode: 510   score: [-405.93626486]   mean: -493.63994794297525\n",
            "episode: 511   score: [-138.35225022]   mean: -467.33867166453456\n",
            "episode: 512   score: [-549.20450583]   mean: -465.9774986734092\n",
            "episode: 513   score: [-132.54090461]   mean: -452.5144977759816\n",
            "episode: 514   score: [-260.98376224]   mean: -465.9579191778297\n",
            "episode: 515   score: [-134.74827377]   mean: -389.8237270627836\n",
            "episode: 516   score: [-127.66611561]   mean: -361.4193024946328\n",
            "episode: 517   score: [-536.41789988]   mean: -361.69298797932663\n",
            "episode: 518   score: [-531.37975148]   mean: -321.16528296537354\n",
            "episode: 519   score: [-567.4270715]   mean: -338.4656800015355\n",
            "episode: 520   score: [-0.78852307]   mean: -297.9509058219798\n",
            "episode: 521   score: [-717.3276881]   mean: -355.84844960952046\n",
            "episode: 522   score: [-532.78751808]   mean: -354.20675083472804\n",
            "episode: 523   score: [-265.22372747]   mean: -367.47503312129675\n",
            "episode: 524   score: [-265.82601204]   mean: -367.9592581014076\n",
            "episode: 525   score: [-130.87310754]   mean: -367.57174147854727\n",
            "episode: 526   score: [-261.95289948]   mean: -381.0004198653829\n",
            "episode: 527   score: [-555.34369352]   mean: -382.892999229448\n",
            "episode: 528   score: [-135.10053561]   mean: -343.26507764290074\n",
            "episode: 529   score: [-974.15072593]   mean: -383.93744308609047\n",
            "episode: 530   score: [-265.68116728]   mean: -410.4267075069026\n",
            "episode: 531   score: [-400.07123008]   mean: -378.70106170473855\n",
            "episode: 532   score: [-550.34202969]   mean: -380.4565128652136\n",
            "episode: 533   score: [-680.06335835]   mean: -421.9404759532172\n",
            "episode: 534   score: [-7.55438138]   mean: -396.1133128867933\n",
            "episode: 535   score: [-0.55393013]   mean: -383.0813951459759\n",
            "episode: 536   score: [-129.87981304]   mean: -369.8740865014229\n",
            "episode: 537   score: [-268.53189916]   mean: -341.1929070652527\n",
            "episode: 538   score: [-398.64094192]   mean: -367.5469476953511\n",
            "episode: 539   score: [-128.31475961]   mean: -282.96335106329883\n",
            "episode: 540   score: [-693.95437405]   mean: -325.79067174072514\n",
            "episode: 541   score: [-137.6547816]   mean: -299.54902689343817\n",
            "episode: 542   score: [-0.32230974]   mean: -244.54705489885478\n",
            "episode: 543   score: [-130.54922376]   mean: -189.59564143930953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}